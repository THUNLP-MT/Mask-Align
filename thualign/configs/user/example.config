[DEFAULT]

; change these to your data paths
datadir = /data/private/cc/data
train_input = ['${datadir}/train/deen/deen.train.src', '${datadir}/train/deen/deen.train.tgt']
valid_input = ['${datadir}/train/deen/deen.valid.src', '${datadir}/train/deen/deen.valid.tgt']
vocab = ['${datadir}/train/deen/vocab.deen.src.txt', '${datadir}/train/deen/vocab.deen.tgt.txt']
test_input = ['${datadir}/test/deen.lc.src.bpe', '${datadir}/test/deen.lc.tgt.bpe']
test_ref = ${datadir}/test/deen.talp

; change these to your experiment directory
; all results are saved in the directory [output]
exp_dir = exp
label = agree_deen
output = ${exp_dir}/${label}

; change these to match your own machine configuration
; the actual training 36k = batch_size * update_cycle * len(device_list)
batch_size = 9000
update_cycle = 1
device_list = [0,1,2,3]
half = True

; model configuration
; the hyperparameters [alpha, beta, lamb, th] are same as described in the paper
model = mask_align
agree_alpha = 5
entropy_loss = True
entropy_beta = 1
renorm_lamb = 0.05
extract_th = 0.2
eval_plot = True

[small_budget]
exp_dir = small_budget_exp
label = small_budget
output = ${exp_dir}/${label}

batch_size = 4500
update_cycle = 8
device_list = [0]
half = False